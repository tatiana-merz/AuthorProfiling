{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "harmful-constraint",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "coastal-convert",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('test_set_tweets.txt', 'r', encoding=\"utf-8\")\n",
    "lines = f.readlines()\n",
    "\n",
    "f_joy = open('joy_words.txt', 'r', encoding=\"utf-8\")\n",
    "joy_word = f_joy.readlines()\n",
    "\n",
    "wr_directory = 'joy_tweets_testset.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "african-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_orig = []\n",
    "sents_cleaned = []\n",
    "joy_words = []\n",
    "test_sets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mexican-accounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "for joy in joy_word:\n",
    "    joy = joy.strip().lower()\n",
    "    joy_words.append(joy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "alleged-memphis",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in lines:\n",
    "    try:\n",
    "        line = line.strip().split('\\t')[2]\n",
    "        sents_orig.append(line)\n",
    "    except (IndexError):\n",
    "        pass\n",
    "#sents_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dramatic-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = r'\\@.+?\\s'\n",
    "html = r'(www|http:|https:)[^\\s]+[\\w]'\n",
    "punct = r'[!\"\\\\#\\\\$%\\\\&\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\.\\/:;<=>\\\\?@\\\\[\\\\\\\\\\\\\\]\\\\^_`\\\\{\\\\|\\\\}\\\\~]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "broke-population",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in sents_orig:\n",
    "    line = re.sub(html, \"\" , line)\n",
    "    line = re.sub(indx, \"\" , line)\n",
    "    line = re.sub(punct, \"\" , line)\n",
    "    line = re.sub(\"\\s+\", \" \", line)\n",
    "    sents_cleaned.append(line)\n",
    "#sents_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hazardous-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict()\n",
    "for sents in sents_cleaned:\n",
    "    sents = sents.strip().split(\" \")\n",
    "    for word in sents:\n",
    "        if word in joy_words:\n",
    "            d[tuple(sents)] = 'joy_word'\n",
    "            #d[tuple(sents)] = word\n",
    "        elif word not in joy_words:\n",
    "            d[tuple(sents)] = 'no_joy_word'\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "immune-outline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 180 joy sentences in the set\n",
      "there are 31673 no joy sentences in the set\n"
     ]
    }
   ],
   "source": [
    "joy = \"there are \" + str(sum(value == \"joy_word\" for value in d.values())) + \" joy sentences in the set\"\n",
    "no_joy = \"there are \" + str(sum(value == \"no_joy_word\" for value in d.values())) + \" no joy sentences in the set\"\n",
    "print(joy)\n",
    "print(no_joy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "scientific-staff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(wr_directory, 'w', encoding=\"utf-8\") as out:\n",
    "    for key, val in d.items():\n",
    "        test_set = '({}, \\'{}\\'),\\n'.format(list(key),val)\n",
    "        test_sets.append(test_set)\n",
    "        out.write(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "short-plumbing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (['Ok', 'today', 'I', 'have', 'to', 'find', 'something', 'to', 'wear', 'for', 'fri', 'cuz', 'I', \"don't\", 'think', 'I', 'have', 'time', 'any', 'other', 'day', 'this', 'week', \"I'm\", 'thinking', 'all', 'black', 'and', 'pearls'], 'no_joy_word'),\n",
      "2 (['I', 'am', 'glad', \"I'm\", 'having', 'this', 'show', 'but', 'I', \"can't\", 'wait', 'till', 'it', 'is', 'over', 'so', 'I', 'can', 'rest', 'and', 'stop', 'worrying'], 'no_joy_word'),\n",
      "3 (['Honestly', 'I', \"don't\", 'even', 'know', \"what's\", 'going', 'on', 'anymore'], 'no_joy_word'),\n",
      "4 (['hey', 'sorry', \"I'm\", 'sitting', 'infront', 'of', 'this', 'sewing', 'mching', 'should', 'be', 'calling', 'u', 'soon'], 'no_joy_word'),\n",
      "5 (['Sitting', 'infront', 'of', 'this', 'sewing', 'machine', 'I', \"don't\", 'feel', 'like', 'doing', 'this', \"I'm\", 'tired', 'and', 'feeling', 'lazy', 'And', 'bored', 'And', 'lonely'], 'no_joy_word'),\n",
      "6 (['Finally', 'home', 'alone', 'Time', 'to', 'sing', 'and', 'dance', 'around', 'the', 'house'], 'no_joy_word'),\n",
      "7 (['Sadly', 'I', \"don't\", 'own', 'a', 'pic', 'of', 'me', 'where', \"I'm\", 'not', 'at', 'the', 'bar', 'I', \"don't\", 'have', 'a', 'drink', 'in', 'my', 'hand', 'or', \"I'm\", 'not', 'slightly', 'intoxicated', 'Lol'], 'no_joy_word'),\n",
      "8 (['chris', 'and', 'I', \"aren't\", 'talking', 'still'], 'no_joy_word'),\n",
      "9 (['NO', 'U', \"don't\"], 'no_joy_word'),\n",
      "10 (['OMG', 'the', 'sun', 'is', 'out', 'I', 'just', 'jumped', 'up', 'and', 'down', 'like', 'a', 'lil', 'kid', 'Its', 'still', 'cold', 'though', 'Lol'], 'no_joy_word'),\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for line in test_sets:\n",
    "    if i <= 10:\n",
    "        print(i, line.strip())\n",
    "        i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
