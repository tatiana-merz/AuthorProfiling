{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "complicated-dining",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from nltk.probability import FreqDist, DictionaryProbDist, ELEProbDist, sum_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "prepared-scenario",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using data frames to read bag of joy words\n",
    "df = pd.read_csv(\"joy_words.txt\", encoding=\"utf-8\", index_col=None , names=[\"joy_words\"])\n",
    "joy_words = df[\"joy_words\"].str.lower().to_list()\n",
    "\n",
    "wr_directory = 'joy_tweets_testset_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "joint-debate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>sentence</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60730027</td>\n",
       "      <td>6320951896</td>\n",
       "      <td>@thediscovietnam coo.  thanks. just dropped yo...</td>\n",
       "      <td>2009-12-03 18:41:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60730027</td>\n",
       "      <td>6320673258</td>\n",
       "      <td>@thediscovietnam shit it ain't lettin me DM yo...</td>\n",
       "      <td>2009-12-03 18:31:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60730027</td>\n",
       "      <td>6319871652</td>\n",
       "      <td>@thediscovietnam hey cody, quick question...ca...</td>\n",
       "      <td>2009-12-03 18:01:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60730027</td>\n",
       "      <td>6318151501</td>\n",
       "      <td>@smokinvinyl dang.  you need anything?  I got ...</td>\n",
       "      <td>2009-12-03 17:00:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60730027</td>\n",
       "      <td>6317932721</td>\n",
       "      <td>maybe i'm late in the game on this one, but th...</td>\n",
       "      <td>2009-12-03 16:52:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60730027</td>\n",
       "      <td>6317701252</td>\n",
       "      <td>i really hope A.I. makes the most of this seco...</td>\n",
       "      <td>2009-12-03 16:44:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>60730027</td>\n",
       "      <td>6315879930</td>\n",
       "      <td>@smokinvinyl danny boy! wanna check out d-nice...</td>\n",
       "      <td>2009-12-03 15:39:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>60730027</td>\n",
       "      <td>6314008277</td>\n",
       "      <td>...and if you have ppl that you care about, ma...</td>\n",
       "      <td>2009-12-03 14:26:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>60730027</td>\n",
       "      <td>6313959103</td>\n",
       "      <td>...that shit weighs heavy on me.  take respons...</td>\n",
       "      <td>2009-12-03 14:24:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>60730027</td>\n",
       "      <td>6313919319</td>\n",
       "      <td>...including his last failed relationship.  an...</td>\n",
       "      <td>2009-12-03 14:23:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id   timestamp                                           sentence  \\\n",
       "0  60730027  6320951896  @thediscovietnam coo.  thanks. just dropped yo...   \n",
       "1  60730027  6320673258  @thediscovietnam shit it ain't lettin me DM yo...   \n",
       "2  60730027  6319871652  @thediscovietnam hey cody, quick question...ca...   \n",
       "3  60730027  6318151501  @smokinvinyl dang.  you need anything?  I got ...   \n",
       "4  60730027  6317932721  maybe i'm late in the game on this one, but th...   \n",
       "5  60730027  6317701252  i really hope A.I. makes the most of this seco...   \n",
       "6  60730027  6315879930  @smokinvinyl danny boy! wanna check out d-nice...   \n",
       "7  60730027  6314008277  ...and if you have ppl that you care about, ma...   \n",
       "8  60730027  6313959103  ...that shit weighs heavy on me.  take respons...   \n",
       "9  60730027  6313919319  ...including his last failed relationship.  an...   \n",
       "\n",
       "                  time  \n",
       "0  2009-12-03 18:41:07  \n",
       "1  2009-12-03 18:31:01  \n",
       "2  2009-12-03 18:01:51  \n",
       "3  2009-12-03 17:00:16  \n",
       "4  2009-12-03 16:52:36  \n",
       "5  2009-12-03 16:44:37  \n",
       "6  2009-12-03 15:39:10  \n",
       "7  2009-12-03 14:26:33  \n",
       "8  2009-12-03 14:24:40  \n",
       "9  2009-12-03 14:23:07  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading tweets as a df\n",
    "# error_bad_lines to skip bad formatted lines\n",
    "# namses to assign column names after separation\n",
    "original_sentences_df = pd.read_csv(\n",
    "\"tweets_trainset_short.txt\", \n",
    "    sep=\"\\t\", \n",
    "    error_bad_lines=False, \n",
    "    encoding=\"utf-8\",\n",
    "    index_col=None,\n",
    "    names=[\"id\", \"timestamp\", \"sentence\", \"time\"])\n",
    "original_sentences_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "attached-architect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex patterns\n",
    "indx = r'\\@.+?\\s'\n",
    "html = r'(www|http:|https:)[^\\s]+[\\w]'\n",
    "punct = r'[!\"\\\\#\\\\$%\\\\&\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\.\\/:;<=>\\\\?@\\\\[\\\\\\\\\\\\\\]\\\\^_`\\\\{\\\\|\\\\}\\\\~]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "median-atlanta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining regex patterns\n",
    "replace_patterns = \"|\".join([indx, html, punct])\n",
    "minor_pattern = \"\\s+\"\n",
    "\n",
    "# replacing patterns, cleaning whitespaces, saving column as a string with astype(str)\n",
    "original_sentences_df[\"sentence\"] = original_sentences_df[\"sentence\"].str.replace(replace_patterns, \"\", regex=True)\n",
    "original_sentences_df[\"sentence\"] = original_sentences_df[\"sentence\"].str.replace(minor_pattern, \" \", regex=True)\n",
    "senteces_ready = original_sentences_df[\"sentence\"].astype(str).str.lower().str.strip().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "remarkable-cassette",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [] # combined list of joy and no_joy sentences\n",
    "joy = [] \n",
    "no_joy = []\n",
    "\n",
    "joy_counter = 0\n",
    "no_joy_counter = 0\n",
    "\n",
    "for sents in senteces_ready:\n",
    "    sentences = sents.split(\" \")\n",
    "    x = set(sentences).intersection(joy_words)\n",
    "    if x:\n",
    "        d.append((\" \".join(sentences), 'joy_word'))\n",
    "        joy.append((\" \".join(sentences), 'joy_word'))\n",
    "        joy_counter += 1\n",
    "    else:\n",
    "        d.append((\" \".join(sentences), 'no_joy_word'))\n",
    "        no_joy.append((\" \".join(sentences), 'no_joy_word'))\n",
    "        no_joy_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "hindu-theology",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(wr_directory, 'w', encoding=\"utf-8\") as out:\n",
    "    #p = \"\".join(str(d))\n",
    "    #print(p)\n",
    "    #out.write(\"\".join(str(d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "interim-saskatchewan",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('coo thanks just dropped you a line', 'no_joy_word')\n",
      "1 (\"shit it ain't lettin me dm you back what's your email\", 'no_joy_word')\n",
      "2 ('hey cody quick questioncan you dm me', 'no_joy_word')\n",
      "3 ('dang you need anything i got some left over meds', 'no_joy_word')\n",
      "4 (\"maybe i'm late in the game on this one but this lowender vst is making my apt rumble\", 'no_joy_word')\n",
      "5 (\"i really hope ai makes the most of this second chance in philly i'm glad he's goin home\", 'joy_word')\n",
      "6 ('danny boy wanna check out d-nice at the afex 1 year tonight we could pre-game at mine and walk over', 'no_joy_word')\n",
      "7 ('and if you have ppl that you care about make sure to let them know life is too short to lose friends over bullshit peace and love', 'joy_word')\n",
      "8 (\"that shit weighs heavy on me take responsibility for your life i don't blame anyone for where i am in this world\", 'no_joy_word')\n",
      "9 ('including his last failed relationship and while i know that none of it is grounded in reality and i actually am worried about him', 'no_joy_word')\n",
      "10 (\"soi got a string of texts last night from someone i haven't heard from in a long time blaming me for everything wrong with his life\", 'no_joy_word')\n"
     ]
    }
   ],
   "source": [
    "# print first 10 sentences to test\n",
    "for i, line in enumerate(d):\n",
    "    if i <= 10:\n",
    "        print(i, line)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "communist-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new list for splitted tockens\n",
    "sents = [] \n",
    "\n",
    "# filtering sentences by word size and sentiments\n",
    "for (words, sentiment) in joy + no_joy:\n",
    "    words_filtered = [e.lower() for e in words.split() if len(e) >= 3]\n",
    "    #print(words_filtered)\n",
    "    sents.append((words_filtered, sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "copyrighted-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions creating a dict with words ordered by their appearance frequency\n",
    "def get_words_in_sents(sents):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in sents:\n",
    "      all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "accepting-brand",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = get_word_features(get_words_in_sents(sents))\n",
    "#word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "lovely-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "curious-plaza",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying features to the classifier using the apply_features method and the tweets processed above\n",
    "training_set = nltk.classify.apply_features(extract_features, sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "genetic-amazon",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-8ae8e8ab168d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# classifier training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNaiveBayesClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nltk/classify/naivebayes.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, labeled_featuresets, estimator)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0mfeature_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0;31m# Keep a list of all feature names.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                 \u001b[0mfnames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m# If a feature didn't have a value given for an instance, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# classifier training\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "stupid-vector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting frequency of each label in the training set \n",
    "def train(labeled_featuresets, estimator=ELEProbDist):\n",
    "    label_probdist = estimator(label_freqdist)\n",
    "    feature_probdist = {}\n",
    "    return NaiveBayesClassifier(label_probdist, feature_probdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a9f8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.classify.accuracy(classifier, training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "valid-webster",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no_joy_word'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "sent = 'This is a test sentence'\n",
    "classifier.classify(extract_features(sent.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "right-gabriel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no_joy_word'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "sent = 'I have lost my sunglasses recently'\n",
    "classifier.classify(extract_features(sent.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "453c30fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'joy_word'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "sent = 'Jenny was so excited when we got her a book as a present'\n",
    "classifier.classify(extract_features(sent.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f18ffb88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'joy_word'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "sent = 'Amazon rain forest is the most beautiful place in the world'\n",
    "classifier.classify(extract_features(sent.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e2f27a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
