{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "complicated-dining",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from nltk.probability import FreqDist, DictionaryProbDist, ELEProbDist, sum_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "prepared-scenario",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using data frames to read bag of joy words\n",
    "df = pd.read_csv(\"joy_words.txt\", encoding=\"utf-8\", index_col=None , names=[\"joy_words\"])\n",
    "joy_words = df[\"joy_words\"].str.lower().to_list()\n",
    "\n",
    "wr_directory = 'joy_tweets_trainset.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "joint-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading tweets as a df\n",
    "# error_bad_lines to skip bad formatted lines\n",
    "# namses to assign column names after separation\n",
    "original_sentences_df = pd.read_csv(\n",
    "\"tweets_trainset_short.txt\", \n",
    "    sep=\"\\t\", \n",
    "    error_bad_lines=False, \n",
    "    encoding=\"utf-8\",\n",
    "    index_col=None,\n",
    "    names=[\"id\", \"timestamp\", \"sentence\", \"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "attached-architect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex patterns\n",
    "indx = r'\\@.+?\\s'\n",
    "html = r'(www|http:|https:)[^\\s]+[\\w]'\n",
    "punct = r'[!\"\\\\#\\\\$%\\\\&\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\.\\/:;<=>\\\\?@\\\\[\\\\\\\\\\\\\\]\\\\^_`\\\\{\\\\|\\\\}\\\\~]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "median-atlanta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining regex patterns\n",
    "replace_patterns = \"|\".join([indx, html, punct])\n",
    "minor_pattern = \"\\s+\"\n",
    "\n",
    "# replacing patterns, cleaning whitespaces, saving column as a string with astype(str)\n",
    "original_sentences_df[\"sentence\"] = original_sentences_df[\"sentence\"].str.replace(replace_patterns, \"\", regex=True)\n",
    "original_sentences_df[\"sentence\"] = original_sentences_df[\"sentence\"].str.replace(minor_pattern, \" \", regex=True)\n",
    "senteces_ready = original_sentences_df[\"sentence\"].astype(str).str.lower().str.strip().to_list()\n",
    "#senteces_ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "remarkable-cassette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there're 26968 examples of joy sentiment\n",
      "there're 72178 examples of no joy sentiment\n"
     ]
    }
   ],
   "source": [
    "d = [] # combined list of joy and no_joy sentences\n",
    "joy = [] \n",
    "no_joy = []\n",
    "\n",
    "joy_counter = 0\n",
    "no_joy_counter = 0\n",
    "\n",
    "for sents in senteces_ready:\n",
    "    sentences = sents.split(\" \")\n",
    "    x = set(sentences).intersection(joy_words)\n",
    "    if x:\n",
    "        d.append((\" \".join(sentences), 'joy_sentence'))\n",
    "        joy.append((\" \".join(sentences), 'joy_sentence'))\n",
    "        joy_counter += 1\n",
    "    else:\n",
    "        d.append((\" \".join(sentences), 'no_joy_sentence'))\n",
    "        no_joy.append((\" \".join(sentences), 'no_joy_sentence'))\n",
    "        no_joy_counter += 1\n",
    "        \n",
    "print(f\"there're {joy_counter} examples of joy sentiment\")\n",
    "print(f\"there're {no_joy_counter} examples of no joy sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hindu-theology",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(wr_directory, 'w', encoding=\"utf-8\") as out:\n",
    "    #p = \"\".join(str(d))\n",
    "    #print(p)\n",
    "    #out.write(\"\".join(str(d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "interim-saskatchewan",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print first 10 sentences to test\n",
    "for i, line in enumerate(d):\n",
    "    if i <= 10:\n",
    "        #print(i, line)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "communist-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new list for splitted tockens\n",
    "sents = [] \n",
    "\n",
    "# filtering sentences by word size and sentiments\n",
    "for (words, sentiment) in joy + no_joy:\n",
    "    words_filtered = [e.lower() for e in words.split() if len(e) >= 3]\n",
    "    #print(words_filtered)\n",
    "    sents.append((words_filtered, sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "powerful-louisville",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions creating a dict with words ordered by their appearance frequency\n",
    "def get_words_in_sents(sents):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in sents:\n",
    "      all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "extended-vector",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = get_word_features(get_words_in_sents(sents))\n",
    "#word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "civilian-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "every-application",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying features to the classifier using the apply_features method and the tweets processed above\n",
    "training_set = nltk.classify.apply_features(extract_features, sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier training\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-asthma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting frequency of each label in the training set \n",
    "def train(labeled_featuresets, estimator=ELEProbDist):\n",
    "    label_probdist = estimator(label_freqdist)\n",
    "    feature_probdist = {}\n",
    "    return NaiveBayesClassifier(label_probdist, feature_probdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "sent = 'I wish to pass Java 2 course in the next semester'\n",
    "classifier.classify(extract_features(sent.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-intent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-carpet",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
